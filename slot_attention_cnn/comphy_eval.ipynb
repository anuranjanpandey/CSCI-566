{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":928022,"sourceType":"datasetVersion","datasetId":500921},{"sourceId":8159884,"sourceType":"datasetVersion","datasetId":4827503},{"sourceId":8169685,"sourceType":"datasetVersion","datasetId":4834693}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport json\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate\nimport argparse\nfrom tqdm import tqdm\nimport time\nimport datetime\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-20T04:45:30.989796Z","iopub.execute_input":"2024-04-20T04:45:30.990202Z","iopub.status.idle":"2024-04-20T04:45:37.611183Z","shell.execute_reply.started":"2024-04-20T04:45:30.990172Z","shell.execute_reply":"2024-04-20T04:45:37.610149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLEVR(Dataset):\n    def __init__(self, root, split='train'):\n        super(CLEVR, self,).__init__()\n        \n        assert split in ['train', 'val', 'test']\n        self.split = split\n        self.root_dir = root  \n        self.files = os.listdir(os.path.join(self.root_dir, 'images', self.split))\n        self.transform = transforms.Compose([\n               transforms.ToTensor()])\n\n    def __getitem__(self, index):\n        path = self.files[index]\n        image = Image.open(os.path.join(self.root_dir, 'images', self.split, path)).convert('RGB')\n        image = image.resize((128 , 128))\n        image = self.transform(image)\n        sample = {'image': image}\n\n        return sample\n            \n    \n    def __len__(self):\n        return len(self.files)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:45:37.613182Z","iopub.execute_input":"2024-04-20T04:45:37.613873Z","iopub.status.idle":"2024-04-20T04:45:37.622740Z","shell.execute_reply.started":"2024-04-20T04:45:37.613840Z","shell.execute_reply":"2024-04-20T04:45:37.621597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass SlotAttention(nn.Module):\n    def __init__(self, num_slots, dim, iters = 3, eps = 1e-8, hidden_dim = 128):\n        super().__init__()\n        self.num_slots = num_slots\n        self.iters = iters\n        self.eps = eps\n        self.scale = dim ** -0.5\n\n        self.slots_mu = nn.Parameter(torch.randn(1, 1, dim))\n        self.slots_sigma = nn.Parameter(torch.rand(1, 1, dim))\n\n        self.to_q = nn.Linear(dim, dim)\n        self.to_k = nn.Linear(dim, dim)\n        self.to_v = nn.Linear(dim, dim)\n\n        self.gru = nn.GRUCell(dim, dim)\n\n        hidden_dim = max(dim, hidden_dim)\n\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, dim)\n\n        self.norm_input  = nn.LayerNorm(dim)\n        self.norm_slots  = nn.LayerNorm(dim)\n        self.norm_pre_ff = nn.LayerNorm(dim)\n\n    def forward(self, inputs, num_slots = None):\n        b, n, d = inputs.shape\n        n_s = num_slots if num_slots is not None else self.num_slots\n        \n        mu = self.slots_mu.expand(b, n_s, -1)\n        sigma = self.slots_sigma.expand(b, n_s, -1)\n        slots = torch.normal(mu, sigma)\n\n        inputs = self.norm_input(inputs)        \n        k, v = self.to_k(inputs), self.to_v(inputs)\n\n        for _ in range(self.iters):\n            slots_prev = slots\n\n            slots = self.norm_slots(slots)\n            q = self.to_q(slots)\n\n            dots = torch.einsum('bid,bjd->bij', q, k) * self.scale\n            attn = dots.softmax(dim=1) + self.eps\n            attn = attn / attn.sum(dim=-1, keepdim=True)\n\n            updates = torch.einsum('bjd,bij->bid', v, attn)\n\n            slots = self.gru(\n                updates.reshape(-1, d),\n                slots_prev.reshape(-1, d)\n            )\n\n            slots = slots.reshape(b, -1, d)\n            slots = slots + self.fc2(F.relu(self.fc1(self.norm_pre_ff(slots))))\n\n        return slots\n\ndef build_grid(resolution):\n    ranges = [np.linspace(0., 1., num=res) for res in resolution]\n    grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n    grid = np.stack(grid, axis=-1)\n    grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n    grid = np.expand_dims(grid, axis=0)\n    grid = grid.astype(np.float32)\n    return torch.from_numpy(np.concatenate([grid, 1.0 - grid], axis=-1)).to(device)\n\n\"\"\"Adds soft positional embedding with learnable projection.\"\"\"\nclass SoftPositionEmbed(nn.Module):\n    def __init__(self, hidden_size, resolution):\n        \"\"\"Builds the soft position embedding layer.\n        Args:\n        hidden_size: Size of input feature dimension.\n        resolution: Tuple of integers specifying width and height of grid.\n        \"\"\"\n        super().__init__()\n        self.embedding = nn.Linear(4, hidden_size, bias=True)\n        self.grid = nn.Parameter(build_grid(resolution), requires_grad=False)\n\n    def forward(self, inputs):\n        grid = self.embedding(self.grid)\n        return inputs + grid\n\nclass Encoder(nn.Module):\n    def __init__(self, resolution, hid_dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, hid_dim, 5, padding = 2)\n        self.conv2 = nn.Conv2d(hid_dim, hid_dim, 5, padding = 2)\n        self.conv3 = nn.Conv2d(hid_dim, hid_dim, 5, padding = 2)\n        self.conv4 = nn.Conv2d(hid_dim, hid_dim, 5, padding = 2)\n        self.encoder_pos = SoftPositionEmbed(hid_dim, resolution)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.conv3(x)\n        x = F.relu(x)\n        x = self.conv4(x)\n        x = F.relu(x)\n        x = x.permute(0,2,3,1)\n        x = self.encoder_pos(x)\n        x = torch.flatten(x, 1, 2)\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, hid_dim, resolution):\n        super().__init__()\n        self.conv1 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(2, 2), padding=2, output_padding=1).to(device)\n        self.conv2 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(2, 2), padding=2, output_padding=1).to(device)\n        self.conv3 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(2, 2), padding=2, output_padding=1).to(device)\n        self.conv4 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(2, 2), padding=2, output_padding=1).to(device)\n        self.conv5 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(1, 1), padding=2).to(device)\n        self.conv6 = nn.ConvTranspose2d(hid_dim, 4, 3, stride=(1, 1), padding=1)\n        self.decoder_initial_size = (8, 8)\n        self.decoder_pos = SoftPositionEmbed(hid_dim, self.decoder_initial_size)\n        self.resolution = resolution\n\n    def forward(self, x):\n        x = self.decoder_pos(x)\n        x = x.permute(0,3,1,2)\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n#         x = F.pad(x, (4,4,4,4)) # no longer needed\n        x = self.conv3(x)\n        x = F.relu(x)\n        x = self.conv4(x)\n        x = F.relu(x)\n        x = self.conv5(x)\n        x = F.relu(x)\n        x = self.conv6(x)\n        x = x[:,:,:self.resolution[0], :self.resolution[1]]\n        x = x.permute(0,2,3,1)\n        return x\n\n\"\"\"Slot Attention-based auto-encoder for object discovery.\"\"\"\nclass SlotAttentionAutoEncoder(nn.Module):\n    def __init__(self, resolution, num_slots, num_iterations, hid_dim):\n        \"\"\"Builds the Slot Attention-based auto-encoder.\n        Args:\n        resolution: Tuple of integers specifying width and height of input image.\n        num_slots: Number of slots in Slot Attention.\n        num_iterations: Number of iterations in Slot Attention.\n        \"\"\"\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.resolution = resolution\n        self.num_slots = num_slots\n        self.num_iterations = num_iterations\n\n        self.encoder_cnn = Encoder(self.resolution, self.hid_dim)\n        self.decoder_cnn = Decoder(self.hid_dim, self.resolution)\n\n        self.fc1 = nn.Linear(hid_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, hid_dim)\n\n        self.slot_attention = SlotAttention(\n            num_slots=self.num_slots,\n            dim=hid_dim,\n            iters = self.num_iterations,\n            eps = 1e-8, \n            hidden_dim = 128)\n\n    def forward(self, image):\n        # `image` has shape: [batch_size, num_channels, width, height].\n\n        # Convolutional encoder with position embedding.\n        x = self.encoder_cnn(image)  # CNN Backbone.\n        x = nn.LayerNorm(x.shape[1:]).to(device)(x)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)  # Feedforward network on set.\n        # `x` has shape: [batch_size, width*height, input_size].\n\n        # Slot Attention module.\n        slots = self.slot_attention(x)\n        # `slots` has shape: [batch_size, num_slots, slot_size].\n\n        # \"\"\"Broadcast slot features to a 2D grid and collapse slot dimension.\"\"\".\n        x = slots.reshape((-1, slots.shape[-1])).unsqueeze(1).unsqueeze(2)\n        x = x.repeat((1, 8, 8, 1))\n        \n        # `slots` has shape: [batch_size*num_slots, width_init, height_init, slot_size].\n        x = self.decoder_cnn(x)\n        # `x` has shape: [batch_size*num_slots, width, height, num_channels+1].\n\n        # Undo combination of slot and batch dimension; split alpha masks.\n        recons, masks = x.reshape(image.shape[0], -1, x.shape[1], x.shape[2], x.shape[3]).split([3,1], dim=-1)\n        # `recons` has shape: [batch_size, num_slots, width, height, num_channels].\n        # `masks` has shape: [batch_size, num_slots, width, height, 1].\n\n        # Normalize alpha masks over slots.\n        masks = nn.Softmax(dim=1)(masks)\n        recon_combined = torch.sum(recons * masks, dim=1)  # Recombine image.\n        recon_combined = recon_combined.permute(0,3,1,2)\n        # `recon_combined` has shape: [batch_size, width, height, num_channels].\n\n        return recon_combined, recons, masks, slots","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:45:37.624241Z","iopub.execute_input":"2024-04-20T04:45:37.624574Z","iopub.status.idle":"2024-04-20T04:45:37.701350Z","shell.execute_reply.started":"2024-04-20T04:45:37.624546Z","shell.execute_reply":"2024-04-20T04:45:37.700031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser()\n\nparser.add_argument\nparser.add_argument('--model_dir', default='/kaggle/working/model1.ckpt', type=str, help='where to save models' )\nparser.add_argument('--seed', default=0, type=int, help='random seed')\nparser.add_argument('--batch_size', default=64, type=int)\nparser.add_argument('--num_slots', default=8, type=int, help='Number of slots in Slot Attention.')\nparser.add_argument('--num_iterations', default=3, type=int, help='Number of attention iterations.')\nparser.add_argument('--hid_dim', default=64, type=int, help='hidden dimension size')\nparser.add_argument('--learning_rate', default=0.0004, type=float)\nparser.add_argument('--warmup_steps', default=10000, type=int, help='Number of warmup steps for the learning rate.')\nparser.add_argument('--decay_rate', default=0.5, type=float, help='Rate for the learning rate decay.')\nparser.add_argument('--decay_steps', default=100000, type=int, help='Number of steps for the learning rate decay.')\nparser.add_argument('--num_workers', default=8, type=int, help='number of workers for loading data')\nparser.add_argument('--num_epochs', default=1, type=int, help='number of workers for loading data')\n\nopt, unknown = parser.parse_known_args()\nresolution = (128, 128)\n\ntrain_set = CLEVR('/kaggle/input/clevr-dataset/CLEVR_v1.0', 'train')\nmodel = SlotAttentionAutoEncoder(resolution, opt.num_slots, opt.num_iterations, opt.hid_dim)\n# model.load_state_dict(torch.load('./tmp/model6.ckpt')['model_state_dict'])\n\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\ncriterion = nn.MSELoss()\n\nparams = [{'params': model.parameters()}]\n\ntrain_dataloader = torch.utils.data.DataLoader(train_set, batch_size=opt.batch_size,\n                        shuffle=True, num_workers=opt.num_workers, pin_memory=True)\n\noptimizer = optim.Adam(params, lr=opt.learning_rate)\n\nstart = time.time()\ni = 0\nfor epoch in range(opt.num_epochs):\n    model.train()\n\n    total_loss = 0\n\n    for sample in tqdm(train_dataloader):\n        i += 1\n\n        if i < opt.warmup_steps:\n            learning_rate = opt.learning_rate * (i / opt.warmup_steps)\n        else:\n            learning_rate = opt.learning_rate\n\n        learning_rate = learning_rate * (opt.decay_rate ** (\n            i / opt.decay_steps))\n\n        optimizer.param_groups[0]['lr'] = learning_rate\n        \n        image = sample['image'].to(device)\n        recon_combined, recons, masks, slots = model(image)\n        loss = criterion(recon_combined, image)\n        total_loss += loss.item()\n\n        del recons, masks, slots\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    total_loss /= len(train_dataloader)\n\n    print (\"Epoch: {}, Loss: {}, Time: {}\".format(epoch, total_loss,\n        datetime.timedelta(seconds=time.time() - start)))\n\n    if not epoch % 10:\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            }, opt.model_dir)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:00:51.667165Z","iopub.execute_input":"2024-04-20T01:00:51.668079Z","iopub.status.idle":"2024-04-20T01:03:12.541452Z","shell.execute_reply.started":"2024-04-20T01:00:51.668044Z","shell.execute_reply":"2024-04-20T01:03:12.539908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport cv2\nfrom transformers import CLIPProcessor, CLIPModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the pre-trained model and processor\nencoder_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\nencoder_model.to(device)\n\nclass COMPHY(Dataset):    \n    def __init__(self, root):\n        super(COMPHY, self,).__init__()\n        self.root_dir = root\n        self.files = list(Path(root).rglob(\"*.mp4\"))\n        self.process = process_video\n\n    def __getitem__(self, index):\n        path = self.files[index]\n        frame_array = self.process(path)\n        items = str(path).split('/')\n        name = items[6].split('.')[0]\n        return np.array(frame_array), name\n    \n    def __len__(self):\n        return len(self.files)\n    \n# Convert video in path to array of frames at rate of 2 FPS\ndef process_video(path):\n    KPS = 2 # Target Keyframes Per Second\n    frame_array = []\n    vidObj = cv2.VideoCapture(str(path))\n    success = 1\n    count = 0\n    fps = round(vidObj.get(cv2.CAP_PROP_FPS))\n    hop = round(fps / KPS)\n    transform = transforms.Compose([transforms.ToTensor()])\n    while(success):\n        success,img = vidObj.read()\n        if img is not None and count % hop == 0:\n            resized = cv2.resize(img, (128, 128))\n            im = Image.fromarray(np.uint8(resized)).convert('RGB')\n            im = transform(im)\n            frame_array.append(im)\n        count += 1\n    return frame_array","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:45:47.005260Z","iopub.execute_input":"2024-04-20T04:45:47.005983Z","iopub.status.idle":"2024-04-20T04:46:12.013018Z","shell.execute_reply.started":"2024-04-20T04:45:47.005952Z","shell.execute_reply":"2024-04-20T04:46:12.012099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = COMPHY('/kaggle/input/comphy/target')\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image as Image, ImageEnhance\n# Hyperparameters.\nseed = 0\nbatch_size = 1\nnum_slots = 8\nnum_iterations = 3\nresolution = (128, 128)\n\nresolution = (128, 128)\nmodel = SlotAttentionAutoEncoder(resolution, num_slots, num_iterations, 64)\nmodel.load_state_dict(torch.load('/kaggle/input/modelcheckpnt/model.ckpt')['model_state_dict'], strict=False)\nmodel.to(device)\n\nslot_array = {}\nfor video, name in dataset:\n    video_slot = []\n    for frame in video:\n        image = torch.from_numpy(frame)\n        image = image.unsqueeze(0).to(device)\n        _,_,_,slots = model(image)\n        video_slot.append(slots.cpu().detach().numpy())\n    slot_array[name] = np.array(video_slot)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:46:56.569260Z","iopub.execute_input":"2024-04-20T04:46:56.570189Z","iopub.status.idle":"2024-04-20T04:47:19.218140Z","shell.execute_reply.started":"2024-04-20T04:46:56.570154Z","shell.execute_reply":"2024-04-20T04:47:19.217097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('slow_out.pkl','wb') as f:\n    pickle.dump(slot_array, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:47:19.219996Z","iopub.execute_input":"2024-04-20T04:47:19.220360Z","iopub.status.idle":"2024-04-20T04:47:19.225918Z","shell.execute_reply.started":"2024-04-20T04:47:19.220331Z","shell.execute_reply":"2024-04-20T04:47:19.224742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for video in slot_array:\n    with open(video+'_slow.pkl','wb') as f:\n        pickle.dump(slot_array[video], f)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:47:19.227244Z","iopub.execute_input":"2024-04-20T04:47:19.227724Z","iopub.status.idle":"2024-04-20T04:47:19.245111Z","shell.execute_reply.started":"2024-04-20T04:47:19.227672Z","shell.execute_reply":"2024-04-20T04:47:19.244328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = CLEVR('/kaggle/input/clevr-dataset/CLEVR_v1.0', 'test')\n\n# image = test_set[2]['image']\nframes, name = dataset[0]\nfor frame in frames:\n    image = torch.from_numpy(frame)\n    image = image.unsqueeze(0).to(device)\n    recon_combined, recons, masks, slots = model(image)\n    fig, ax = plt.subplots(1, num_slots + 2, figsize=(15, 2))\n    image = image.squeeze(0)\n    recon_combined = recon_combined.squeeze(0)\n    recons = recons.squeeze(0)\n    masks = masks.squeeze(0)\n    image = image.permute(1,2,0).cpu().numpy()\n    recon_combined = recon_combined.permute(1,2,0)\n    recon_combined = recon_combined.cpu().detach().numpy()\n    recons = recons.cpu().detach().numpy()\n    masks = masks.cpu().detach().numpy()\n    ax[0].imshow(image)\n    ax[0].set_title('Image')\n    ax[1].imshow(recon_combined)\n    ax[1].set_title('Recon.')\n    for i in range(num_slots):\n        picture = recons[i] * masks[i] + (1 - masks[i])\n        ax[i + 2].imshow(picture)\n        ax[i + 2].set_title('Slot %s' % str(i + 1))\n    for i in range(len(ax)):\n        ax[i].grid(False)\n        ax[i].axis('off')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:47:19.247348Z","iopub.execute_input":"2024-04-20T04:47:19.247758Z","iopub.status.idle":"2024-04-20T04:47:29.646818Z","shell.execute_reply.started":"2024-04-20T04:47:19.247722Z","shell.execute_reply":"2024-04-20T04:47:29.645695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}